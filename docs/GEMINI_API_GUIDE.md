# Google Generative AI (Gemini) API ä½¿ç”¨æ•™å­¸

> æœ¬æ–‡ä»¶çµåˆå¯¦éš›å°ˆæ¡ˆ (RAG_by_Google) çš„æ‡‰ç”¨æ¡ˆä¾‹ï¼Œå®Œæ•´ä»‹ç´¹ Google Generative AI SDK çš„ä½¿ç”¨æ–¹æ³•

---

## ğŸ“š ç›®éŒ„

1. [åŸºç¤è¨­ç½®](#1-åŸºç¤è¨­ç½®)
2. [æ¨¡å‹ç®¡ç†](#2-æ¨¡å‹ç®¡ç†)
3. [æ–‡ä»¶ç®¡ç†](#3-æ–‡ä»¶ç®¡ç†)
4. [å…§å®¹ç”Ÿæˆ](#4-å…§å®¹ç”Ÿæˆ)
5. [Embedding åµŒå…¥å‘é‡](#5-embedding-åµŒå…¥å‘é‡)
6. [é€²éšæ‡‰ç”¨](#6-é€²éšæ‡‰ç”¨)
7. [æœ€ä½³å¯¦è¸](#7-æœ€ä½³å¯¦è¸)
8. [å¸¸è¦‹å•é¡Œ](#8-å¸¸è¦‹å•é¡Œ)

---

## 1. åŸºç¤è¨­ç½®

### 1.1 å®‰è£ SDK

```bash
pip install google-generativeai
```

### 1.2 API Key é…ç½®

#### æ–¹æ³• Aï¼šç’°å¢ƒè®Šæ•¸ï¼ˆæ¨è–¦ï¼‰

```python
import os
import google.generativeai as genai

# å¾ç’°å¢ƒè®Šæ•¸è®€å–
api_key = os.getenv('GOOGLE_API_KEY')
genai.configure(api_key=api_key)
```

#### æ–¹æ³• Bï¼šç›´æ¥è¨­å®š

```python
import google.generativeai as genai

genai.configure(api_key='YOUR_API_KEY_HERE')
```

### ğŸ“Œ å°ˆæ¡ˆå¯¦ä½œåƒè€ƒ

**æª”æ¡ˆï¼š** `backend/services/rag_service.py` (ç¬¬ 16-29 è¡Œ)

```python
class RAGService:
    def __init__(self, api_key: str):
        """åˆå§‹åŒ– RAG æœå‹™"""
        if not api_key:
            raise ValueError("API key is required")
        
        self.api_key = api_key
        self.logger = get_logger(__name__)
        
        # é…ç½® legacy APIï¼ˆå…¼å®¹æ€§ï¼‰
        genai.configure(api_key=api_key)
        
        # åˆå§‹åŒ–æ–°ç‰ˆ clientï¼ˆç”¨æ–¼åˆ—å‡ºæ¨¡å‹ï¼‰
        self.client = genai_client.Client(api_key=api_key)
        
        self.logger.info(f"API Key loaded: {api_key[:10]}...")
```

**èªªæ˜ï¼š**
- æœ¬å°ˆæ¡ˆåŒæ™‚ä½¿ç”¨å…©å€‹ API ç‰ˆæœ¬ï¼š`google.generativeai` å’Œ `google.genai`
- `genai.configure()` ç”¨æ–¼èˆŠç‰ˆ APIï¼ˆæ–‡ä»¶ä¸Šå‚³ã€å…§å®¹ç”Ÿæˆï¼‰
- `genai_client.Client()` ç”¨æ–¼æ–°ç‰ˆ APIï¼ˆæ¨¡å‹åˆ—è¡¨ï¼‰

---

## 2. æ¨¡å‹ç®¡ç†

### 2.1 åˆ—å‡ºå¯ç”¨æ¨¡å‹

```python
from google import genai

client = genai.Client(api_key='YOUR_API_KEY')

for model in client.models.list():
    print(f"Model: {model.name}")
    print(f"Display Name: {model.display_name}")
    print(f"Supported Actions: {model.supported_actions}")
    print("---")
```

### 2.2 æ¨¡å‹é¸æ“‡æŒ‡å—

| æ¨¡å‹ ID | ç‰¹é» | é©ç”¨å ´æ™¯ | æˆæœ¬ |
|---------|------|----------|------|
| `gemini-2.0-flash-exp` | å¯¦é©—æ€§æœ€æ–°ç‰ˆ | æ¸¬è©¦æ–°åŠŸèƒ½ | å…è²»ï¼ˆæœ‰é™é¡ï¼‰ |
| `gemini-1.5-pro` | é«˜å“è³ªæ¨ç† | è¤‡é›œé‚è¼¯ã€é•·æ–‡åˆ†æ | é«˜ |
| `gemini-1.5-flash` | é€Ÿåº¦èˆ‡å“è³ªå¹³è¡¡ | ä¸€èˆ¬å°è©±ã€RAG æ‡‰ç”¨ | ä¸­ |
| `gemini-1.0-pro` | ç©©å®šå¯é  | ç”Ÿç”¢ç’°å¢ƒ | ä½ |

### ğŸ“Œ å°ˆæ¡ˆå¯¦ä½œåƒè€ƒ

**æª”æ¡ˆï¼š** `backend/services/rag_service.py` (ç¬¬ 33-89 è¡Œ)

```python
def _load_available_models(self) -> List[Dict[str, str]]:
    """å¾ Google AI API è¼‰å…¥å¯ç”¨æ¨¡å‹"""
    try:
        available_models = []
        
        for model in self.client.models.list():
            # åªé¸æ“‡æ”¯æ´ generateContent çš„ Gemini æ¨¡å‹
            if 'generateContent' in model.supported_actions and 'gemini' in model.name.lower():
                
                model_id = model.name.replace('models/', '')
                description = self._get_model_description(model_id)
                
                available_models.append({
                    'model_id': model_id,
                    'name': model.display_name or model_id,
                    'description': description
                })
        
        # ä¾åç¨±æ’åºï¼Œå„ªå…ˆé¡¯ç¤ºæœ€æ–°ç‰ˆæœ¬
        available_models.sort(key=lambda x: (x['model_id'].replace('-', ''), x['model_id']))
        
        self.logger.info(f"Loaded {len(available_models)} available models")
        return available_models
        
    except Exception as e:
        self.logger.warning(f"Could not load model list: {e}", exc_info=True)
        
        # å¤±æ•—æ™‚è¿”å›åŸºæœ¬æ¨¡å‹æ¸…å–®
        fallback_models = [
            {
                'model_id': 'gemini-1.5-flash',
                'name': 'Gemini 1.5 Flash',
                'description': 'Fast model - Balanced speed and quality'
            }
        ]
        return fallback_models

def _get_model_description(self, model_id: str) -> str:
    """æ ¹æ“šæ¨¡å‹ ID å–å¾—æè¿°"""
    descriptions = {
        'gemini-2.0-flash-exp': 'Latest experimental model - Best performance',
        'gemini-1.5-pro': 'Pro version - Complex reasoning',
        'gemini-1.5-flash': 'Flash version - Balanced speed and quality',
        'gemini-1.0-pro': 'Standard version - Stable and reliable'
    }
    
    # å˜—è©¦ç²¾ç¢ºåŒ¹é…
    if model_id in descriptions:
        return descriptions[model_id]
        
    # æ¨¡ç³ŠåŒ¹é…
    for key, desc in descriptions.items():
        if key in model_id:
            return desc
            
    # é è¨­æè¿°
    if 'pro' in model_id:
        return 'Pro model - High quality output'
    elif 'flash' in model_id:
        return 'Fast model - Efficient response'
    else:
        return 'Standard Gemini model'
```

**é‡é»èªªæ˜ï¼š**
1. **å‹•æ…‹è¼‰å…¥æ¨¡å‹åˆ—è¡¨**ï¼šè‡ªå‹•å–å¾—æœ€æ–°å¯ç”¨æ¨¡å‹
2. **éæ¿¾æ¢ä»¶**ï¼šåªé¸æ“‡æ”¯æ´ `generateContent` ä¸”åç¨±åŒ…å« `gemini` çš„æ¨¡å‹
3. **éŒ¯èª¤è™•ç†**ï¼šAPI å¤±æ•—æ™‚ä½¿ç”¨ fallback æ¨¡å‹æ¸…å–®
4. **ç”¨æˆ¶å‹å–„**ï¼šç‚ºæ¯å€‹æ¨¡å‹æä¾›ä¸­æ–‡æè¿°

---

## 3. æ–‡ä»¶ç®¡ç†

### 3.1 ä¸Šå‚³æ–‡ä»¶

```python
import google.generativeai as genai

# ä¸Šå‚³æœ¬åœ°æ–‡ä»¶
uploaded_file = genai.upload_file(
    path='./data/document.pdf',
    display_name='My Document'
)

print(f"Uploaded file: {uploaded_file.name}")
print(f"URI: {uploaded_file.uri}")
print(f"State: {uploaded_file.state.name}")
```

**æ”¯æ´çš„æ–‡ä»¶æ ¼å¼ï¼š**
- æ–‡å­—ï¼šTXT, MD, CSV
- æ–‡ä»¶ï¼šPDF, DOCX
- åœ–ç‰‡ï¼šPNG, JPEG, WEBP
- å½±ç‰‡ï¼šMP4, MOV
- éŸ³è¨Šï¼šMP3, WAV

### 3.2 åˆ—å‡ºå·²ä¸Šå‚³æ–‡ä»¶

```python
files = genai.list_files()

for file in files:
    print(f"Name: {file.display_name}")
    print(f"Size: {file.size_bytes} bytes")
    print(f"Created: {file.create_time}")
```

### 3.3 åˆªé™¤æ–‡ä»¶

```python
genai.delete_file(file_name='files/abc123xyz')
```

### ğŸ“Œ å°ˆæ¡ˆå¯¦ä½œåƒè€ƒ

**æª”æ¡ˆï¼š** `backend/services/rag_service.py`

#### A. åˆ—å‡ºæ–‡ä»¶ (ç¬¬ 107-129 è¡Œ)

```python
def list_files(self) -> List[Dict[str, Any]]:
    """åˆ—å‡ºæ‰€æœ‰å·²ä¸Šå‚³æ–‡ä»¶"""
    try:
        files = genai.list_files()
        return [
            {
                'name': file.name,
                'display_name': file.display_name,
                'uri': file.uri if hasattr(file, 'uri') else None,
                'size_bytes': file.size_bytes,
                'create_time': file.create_time.isoformat() if file.create_time else None,
                'state': file.state.name if hasattr(file.state, 'name') else str(file.state)
            } 
            for file in files
        ]
    except Exception as e:
        self.logger.error(f"Error listing files: {e}")
        return []
```

#### B. ä¸Šå‚³æ–‡ä»¶ (ç¬¬ 131-156 è¡Œ)

```python
def upload_file(self, file_path: str, display_name: Optional[str] = None) -> Dict[str, Any]:
    """ä¸Šå‚³æ–‡ä»¶åˆ° Gemini"""
    try:
        uploaded_file = genai.upload_file(
            path=file_path,
            display_name=display_name
        )
        
        self.logger.info(f"Successfully uploaded file: {display_name or file_path}")
        
        return {
            'name': uploaded_file.name,
            'display_name': uploaded_file.display_name,
            'uri': uploaded_file.uri if hasattr(uploaded_file, 'uri') else None,
            'size_bytes': uploaded_file.size_bytes,
            'create_time': uploaded_file.create_time.isoformat() if uploaded_file.create_time else None,
            'state': uploaded_file.state.name if hasattr(uploaded_file.state, 'name') else 'ACTIVE'
        }
    except Exception as e:
        self.logger.error(f"Failed to upload file {display_name or file_path}: {e}")
        raise FileUploadError(f"Failed to upload file: {e}")
```

#### C. åˆªé™¤æ–‡ä»¶ (ç¬¬ 158-167 è¡Œ)

```python
def delete_file(self, file_name: str) -> bool:
    """å¾ Gemini åˆªé™¤æ–‡ä»¶"""
    try:
        genai.delete_file(file_name)
        self.logger.info(f"Deleted file: {file_name}")
        return True
    except Exception as e:
        self.logger.error(f"Error deleting file {file_name}: {e}")
        return False
```

#### D. æ‰¹æ¬¡ä¸Šå‚³è³‡æ–™å¤¾ (ç¬¬ 393-444 è¡Œ)

```python
def upload_folder(self, folder_path: str) -> Dict[str, Any]:
    """
    ä¸Šå‚³è³‡æ–™å¤¾å…§æ‰€æœ‰æ–‡ä»¶åˆ° Gemini
    
    Args:
        folder_path: åŒ…å«æ–‡ä»¶çš„è³‡æ–™å¤¾è·¯å¾‘
        
    Returns:
        Dict åŒ…å«ä¸Šå‚³çµæœ
    """
    import glob
    
    uploaded = []
    failed = []
    
    try:
        # å–å¾—æ‰€æœ‰æ”¯æ´çš„æ–‡ä»¶
        file_patterns = [
            os.path.join(folder_path, '*.txt'),
            os.path.join(folder_path, '*.md'),
            os.path.join(folder_path, '*.pdf')
        ]
        
        files_to_upload = []
        for pattern in file_patterns:
            files_to_upload.extend(glob.glob(pattern))
        
        self.logger.info(f"Found {len(files_to_upload)} files to upload from {folder_path}")
        
        for file_path in files_to_upload:
            try:
                display_name = os.path.basename(file_path)
                result = self.upload_file(file_path, display_name)
                uploaded.append(result)
            except Exception as e:
                self.logger.error(f"Failed to upload {file_path}: {e}")
                failed.append({
                    'file_path': file_path,
                    'error': str(e)
                })
        
        return {
            'uploaded': uploaded,
            'uploaded_count': len(uploaded),
            'failed': failed,
            'failed_count': len(failed)
        }
        
    except Exception as e:
        self.logger.error(f"Error uploading folder {folder_path}: {e}")
        return {
            'uploaded': uploaded,
            'uploaded_count': len(uploaded),
            'failed': failed,
            'failed_count': len(failed),
            'error': str(e)
        }
```

**é‡é»èªªæ˜ï¼š**
1. **éŒ¯èª¤è™•ç†**ï¼šæ¯å€‹æ“ä½œéƒ½æœ‰å®Œæ•´çš„ try-except
2. **æ—¥èªŒè¨˜éŒ„**ï¼šè¨˜éŒ„æ‰€æœ‰é‡è¦æ“ä½œ
3. **å±¬æ€§æª¢æŸ¥**ï¼šä½¿ç”¨ `hasattr()` é¿å…å±¬æ€§ä¸å­˜åœ¨éŒ¯èª¤
4. **æ‰¹æ¬¡è™•ç†**ï¼š`upload_folder()` æ”¯æ´æ‰¹æ¬¡ä¸Šå‚³ä¸¦è¨˜éŒ„æˆåŠŸ/å¤±æ•—

---

## 4. å…§å®¹ç”Ÿæˆ

### 4.1 åŸºæœ¬æ–‡å­—ç”Ÿæˆ

```python
import google.generativeai as genai

# åˆå§‹åŒ–æ¨¡å‹
model = genai.GenerativeModel('gemini-1.5-flash')

# ç”Ÿæˆå…§å®¹
response = model.generate_content("ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ")
print(response.text)
```

### 4.2 å¤šæ¨¡æ…‹è¼¸å…¥ï¼ˆæ–‡å­— + æ–‡ä»¶ï¼‰

```python
# å–å¾—å·²ä¸Šå‚³çš„æ–‡ä»¶
file = genai.get_file('files/abc123xyz')

# çµ„åˆæ–‡ä»¶èˆ‡æç¤ºè©
prompt_parts = [
    file,
    "è«‹ç¸½çµé€™ä»½æ–‡ä»¶çš„é‡é»"
]

response = model.generate_content(prompt_parts)
print(response.text)
```

### 4.3 ä¸²æµéŸ¿æ‡‰ï¼ˆStreamingï¼‰

```python
response_stream = model.generate_content(
    "å¯«ä¸€ç¯‡é—œæ–¼ AI çš„æ–‡ç« ",
    stream=True  # å•Ÿç”¨ä¸²æµ
)

for chunk in response_stream:
    if chunk.text:
        print(chunk.text, end='', flush=True)
```

### 4.4 é…ç½®ç”Ÿæˆåƒæ•¸

```python
from google.generativeai.types import GenerationConfig

response = model.generate_content(
    "å‰µä½œä¸€é¦–è©©",
    generation_config=GenerationConfig(
        max_output_tokens=8192,  # æœ€å¤§è¼¸å‡º token æ•¸
        temperature=0.7,         # å‰µé€ æ€§ï¼ˆ0-1ï¼‰
        top_p=0.95,             # æ ¸å¿ƒæ¡æ¨£
        top_k=40                # Top-K æ¡æ¨£
    )
)
```

**åƒæ•¸èªªæ˜ï¼š**
- `temperature`ï¼šæ§åˆ¶éš¨æ©Ÿæ€§ï¼ˆ0 = ç¢ºå®šæ€§ï¼Œ1 = æœ€å¤§å‰µé€ æ€§ï¼‰
- `max_output_tokens`ï¼šé™åˆ¶å›æ‡‰é•·åº¦
- `top_p`ï¼šæ ¸å¿ƒæ¡æ¨£ï¼Œä¿ç•™ç´¯ç©æ©Ÿç‡é”åˆ° p çš„ token
- `top_k`ï¼šåªå¾æ©Ÿç‡æœ€é«˜çš„ k å€‹ token ä¸­æ¡æ¨£

### ğŸ“Œ å°ˆæ¡ˆå¯¦ä½œåƒè€ƒ

**æª”æ¡ˆï¼š** `backend/services/rag_service.py`

#### A. éä¸²æµæŸ¥è©¢ (ç¬¬ 176-259 è¡Œ)

```python
def query(
    self, 
    query: str, 
    model_name: Optional[str] = None, 
    selected_file_names: Optional[List[str]] = None,
    system_prompt: Optional[str] = None,
    max_output_tokens: int = MAX_OUTPUT_TOKENS
) -> Dict[str, Any]:
    """
    ä½¿ç”¨å¯é¸çš„æ–‡ä»¶ä¸Šä¸‹æ–‡å’Œè‡ªè¨‚ç³»çµ±æç¤ºè©æŸ¥è©¢æ¨¡å‹
    
    Args:
        query: ç”¨æˆ¶æŸ¥è©¢æ–‡å­—
        model_name: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé è¨­ç‚º DEFAULT_MODELï¼‰
        selected_file_names: è¦åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶åç¨±åˆ—è¡¨
        system_prompt: è‡ªè¨‚ç³»çµ±æç¤ºè©ï¼ˆNone å‰‡ä½¿ç”¨é è¨­ï¼‰
        max_output_tokens: å›æ‡‰çš„æœ€å¤§ token æ•¸
        
    Returns:
        Dict åŒ…å«æˆåŠŸç‹€æ…‹ã€å›æ‡‰å’Œå…ƒæ•¸æ“š
        
    Raises:
        ModelValidationError: å¦‚æœæ¨¡å‹ä¸å¯ç”¨
    """
    if not model_name:
        model_name = DEFAULT_MODEL
    
    # é©—è­‰æ¨¡å‹
    available_model_ids = [m['model_id'] for m in self.get_available_models()]
    if model_name not in available_model_ids:
        raise ModelValidationError(
            f"Unsupported model: {model_name}. Available models: {', '.join(available_model_ids)}"
        )
    
    try:
        model = genai.GenerativeModel(model_name)
        
        # å»ºç«‹æç¤ºè©å…§å®¹
        prompt_parts = []
        
        # åŠ å…¥é¸å®šçš„æ–‡ä»¶
        files_used = 0
        if selected_file_names:
            all_files = self.list_files()
            file_map = {f['name']: f for f in all_files}
            
            for file_name in selected_file_names:
                if file_name in file_map:
                    # åŠ å…¥æ–‡ä»¶åƒè€ƒ
                    file_obj = genai.get_file(file_name)
                    prompt_parts.append(file_obj)
                    files_used += 1
        
        # åŠ å…¥å¯¦éš›æŸ¥è©¢èˆ‡è‡ªè¨‚æˆ–é è¨­ç³»çµ±æç¤ºè©
        default_system_prompt = """Based on the provided document content, please answer the following question:

{query}

If the documents don't contain relevant information, please state that clearly and provide a general answer."""
        
        final_prompt = system_prompt if system_prompt else default_system_prompt
        prompt_parts.append(final_prompt.format(query=query))
        
        # ç”Ÿæˆå›æ‡‰
        response = model.generate_content(
            prompt_parts,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_output_tokens,
                temperature=0.7,
            )
        )
        
        # å¾å›æ‡‰ä¸­æå– token ä½¿ç”¨é‡
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0
        if hasattr(response, 'usage_metadata'):
            prompt_tokens = getattr(response.usage_metadata, 'prompt_token_count', 0)
            completion_tokens = getattr(response.usage_metadata, 'candidates_token_count', 0)
            total_tokens = getattr(response.usage_metadata, 'total_token_count', 0)
        
        self.logger.info(
            f"Query successful with model {model_name}, "
            f"files: {files_used}, tokens: {total_tokens}"
        )
        
        return {
            'success': True,
            'response': response.text,
            'model_used': model_name,
            'files_used': files_used,
            'system_prompt_used': system_prompt if system_prompt else default_system_prompt.format(query=query),
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens,
            'total_tokens': total_tokens
        }
        
    except Exception as e:
        self.logger.error(f"Query failed with model {model_name}: {e}", exc_info=True)
        return {
            'success': False,
            'response': f"Query failed: {str(e)}",
            'model_used': model_name,
            'files_used': 0,
            'error': str(e)
        }
```

#### B. ä¸²æµæŸ¥è©¢ (ç¬¬ 261-391 è¡Œ)

```python
def query_stream(
    self, 
    query: str, 
    model_name: Optional[str] = None, 
    selected_file_names: Optional[List[str]] = None,
    system_prompt: Optional[str] = None,
    max_output_tokens: int = MAX_OUTPUT_TOKENS
) -> Generator[Dict[str, Any], None, None]:
    """
    ä½¿ç”¨ä¸²æµå›æ‡‰æŸ¥è©¢æ¨¡å‹ - å³æ™‚ç”¢ç”Ÿå›æ‡‰å¡Š
    
    Args:
        query: ç”¨æˆ¶æŸ¥è©¢æ–‡å­—
        model_name: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆé è¨­ç‚º DEFAULT_MODELï¼‰
        selected_file_names: è¦åŒ…å«åœ¨ä¸Šä¸‹æ–‡ä¸­çš„æ–‡ä»¶åç¨±åˆ—è¡¨
        system_prompt: è‡ªè¨‚ç³»çµ±æç¤ºè©ï¼ˆNone å‰‡ä½¿ç”¨é è¨­ï¼‰
        max_output_tokens: å›æ‡‰çš„æœ€å¤§ token æ•¸
        
    Yields:
        Dict åŒ…å«å›æ‡‰å¡Šè³‡æ–™æˆ–éŒ¯èª¤è¨Šæ¯
    """
    if not model_name:
        model_name = DEFAULT_MODEL
    
    # é©—è­‰æ¨¡å‹
    available_model_ids = [m['model_id'] for m in self.get_available_models()]
    if model_name not in available_model_ids:
        self.logger.error(f"Unsupported model in stream: {model_name}")
        yield {
            'type': 'error',
            'error': f"Unsupported model: {model_name}",
            'model_used': model_name
        }
        return
    
    try:
        model = genai.GenerativeModel(model_name)
        
        # å»ºç«‹æç¤ºè©å…§å®¹
        prompt_parts = []
        
        # åŠ å…¥é¸å®šçš„æ–‡ä»¶
        files_used = 0
        if selected_file_names:
            all_files = self.list_files()
            file_map = {f['name']: f for f in all_files}
            
            for file_name in selected_file_names:
                if file_name in file_map:
                    # åŠ å…¥æ–‡ä»¶åƒè€ƒ
                    file_obj = genai.get_file(file_name)
                    prompt_parts.append(file_obj)
                    files_used += 1
        
        # åŠ å…¥å¯¦éš›æŸ¥è©¢èˆ‡è‡ªè¨‚æˆ–é è¨­ç³»çµ±æç¤ºè©
        default_system_prompt = """Based on the provided document content, please answer the following question:

{query}

If the documents don't contain relevant information, please state that clearly and provide a general answer."""
        
        final_prompt = system_prompt if system_prompt else default_system_prompt
        prompt_parts.append(final_prompt.format(query=query))
        
        # ç”Ÿæˆä¸²æµå›æ‡‰
        response_stream = model.generate_content(
            prompt_parts,
            generation_config=genai.types.GenerationConfig(
                max_output_tokens=max_output_tokens,
                temperature=0.7,
            ),
            stream=True  # å•Ÿç”¨ä¸²æµ
        )
        
        # å³æ™‚ç”¢ç”Ÿå›æ‡‰å¡Š
        full_response = ""
        for chunk in response_stream:
            if chunk.text:
                full_response += chunk.text
                yield {
                    'type': 'chunk',
                    'text': chunk.text,
                    'model_used': model_name,
                    'files_used': files_used
                }
        
        # ç™¼é€å®Œæˆè¨Šæ¯èˆ‡ token ä½¿ç”¨é‡
        prompt_tokens = 0
        completion_tokens = 0
        total_tokens = 0
        if hasattr(response_stream, 'usage_metadata'):
            prompt_tokens = getattr(response_stream.usage_metadata, 'prompt_token_count', 0)
            completion_tokens = getattr(response_stream.usage_metadata, 'candidates_token_count', 0)
            total_tokens = getattr(response_stream.usage_metadata, 'total_token_count', 0)
        
        self.logger.info(
            f"Streaming query completed with model {model_name}, tokens: {total_tokens}"
        )
        
        yield {
            'type': 'complete',
            'full_response': full_response,
            'system_prompt_used': system_prompt if system_prompt else default_system_prompt.format(query=query),
            'prompt_tokens': prompt_tokens,
            'completion_tokens': completion_tokens,
            'total_tokens': total_tokens
        }
        
    except Exception as e:
        self.logger.error(f"Streaming query failed with model {model_name}: {e}", exc_info=True)
        yield {
            'type': 'error',
            'error': str(e),
            'model_used': model_name
        }
```

**é‡é»èªªæ˜ï¼š**
1. **RAG æ¨¡å¼**ï¼šçµåˆæ–‡ä»¶ä¸Šä¸‹æ–‡èˆ‡ç”¨æˆ¶æŸ¥è©¢
2. **System Prompt**ï¼šæ”¯æ´è‡ªè¨‚æç¤ºè©ï¼Œé è¨­ä½¿ç”¨åŸºæ–¼æ–‡ä»¶çš„æç¤º
3. **Token çµ±è¨ˆ**ï¼šè¨˜éŒ„ promptã€completion å’Œ total tokens
4. **ä¸²æµæ”¯æ´**ï¼š`query_stream()` ä½¿ç”¨ generator å³æ™‚å›å‚³å…§å®¹
5. **éŒ¯èª¤è™•ç†**ï¼šå®Œæ•´çš„ç•°å¸¸æ•æ‰èˆ‡æ—¥èªŒè¨˜éŒ„

---

## 5. Embedding åµŒå…¥å‘é‡

### 5.1 ç”Ÿæˆæ–‡æœ¬ Embedding

```python
import google.generativeai as genai

# ç”Ÿæˆæ–‡æª” embedding
result = genai.embed_content(
    model='models/text-embedding-004',
    content='æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯',
    task_type='retrieval_document'  # ç”¨æ–¼æ–‡æª”å„²å­˜
)

embedding = result['embedding']  # 768 ç¶­å‘é‡
print(f"Embedding ç¶­åº¦: {len(embedding)}")
```

### 5.2 ç”ŸæˆæŸ¥è©¢ Embedding

```python
# ç”ŸæˆæŸ¥è©¢ embeddingï¼ˆèˆ‡æ–‡æª”æª¢ç´¢ç›¸é—œï¼‰
result = genai.embed_content(
    model='models/text-embedding-004',
    content='ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ',
    task_type='retrieval_query'  # ç”¨æ–¼æŸ¥è©¢
)

query_embedding = result['embedding']
```

### 5.3 Task Types èªªæ˜

| Task Type | ç”¨é€” | èªªæ˜ |
|-----------|------|------|
| `retrieval_document` | æ–‡æª”å„²å­˜ | å°‡æ–‡æª”è½‰æ›ç‚ºå‘é‡ä¸¦å„²å­˜åœ¨è³‡æ–™åº« |
| `retrieval_query` | æŸ¥è©¢æª¢ç´¢ | å°‡æŸ¥è©¢è½‰æ›ç‚ºå‘é‡ç”¨æ–¼æœå°‹ |
| `semantic_similarity` | èªç¾©ç›¸ä¼¼åº¦ | è¨ˆç®—å…©å€‹æ–‡æœ¬çš„ç›¸ä¼¼åº¦ |
| `classification` | åˆ†é¡ | æ–‡æœ¬åˆ†é¡ä»»å‹™ |
| `clustering` | èšé¡ | æ–‡æœ¬èšé¡åˆ†æ |

### 5.4 è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦

```python
import numpy as np

def cosine_similarity(vec1, vec2):
    """è¨ˆç®—å…©å€‹å‘é‡çš„é¤˜å¼¦ç›¸ä¼¼åº¦"""
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    
    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

# ä½¿ç”¨ç¯„ä¾‹
doc_embedding = genai.embed_content(
    model='models/text-embedding-004',
    content='Python æ˜¯ä¸€ç¨®ç¨‹å¼èªè¨€',
    task_type='retrieval_document'
)['embedding']

query_embedding = genai.embed_content(
    model='models/text-embedding-004',
    content='ä»€éº¼æ˜¯ Pythonï¼Ÿ',
    task_type='retrieval_query'
)['embedding']

similarity = cosine_similarity(doc_embedding, query_embedding)
print(f"ç›¸ä¼¼åº¦: {similarity:.4f}")
```

### ğŸ“Œ å°ˆæ¡ˆå¯¦ä½œåƒè€ƒ

**æª”æ¡ˆï¼š** `backend/services/embedding_service.py`

```python
import google.generativeai as genai
from typing import List, Optional
import numpy as np
from backend.config import EMBEDDING_MODEL
from backend.exceptions import EmbeddingError
from backend.utils.logger import get_logger


class EmbeddingService:
    """ä½¿ç”¨ Gemini API ç”Ÿæˆå’Œç®¡ç† embeddings çš„æœå‹™"""
    
    def __init__(self, api_key: str):
        """
        åˆå§‹åŒ– embedding æœå‹™
        
        Args:
            api_key: Google API key ç”¨æ–¼é©—è­‰
        """
        if not api_key:
            raise ValueError("API key is required")
        
        self.api_key = api_key
        self.logger = get_logger(__name__)
        genai.configure(api_key=api_key)
        self.embedding_model = EMBEDDING_MODEL  # models/text-embedding-004
        
        self.logger.info(f"EmbeddingService initialized with model: {self.embedding_model}")
    
    def generate_embedding(self, text: str) -> List[float]:
        """
        ç‚ºçµ¦å®šæ–‡æœ¬ç”Ÿæˆ embedding å‘é‡
        
        Args:
            text: è¦åµŒå…¥çš„æ–‡æœ¬
            
        Returns:
            è¡¨ç¤º embedding å‘é‡çš„æµ®é»æ•¸åˆ—è¡¨ï¼ˆ768 ç¶­ï¼‰
            
        Raises:
            EmbeddingError: å¦‚æœ embedding ç”Ÿæˆå¤±æ•—
        """
        try:
            result = genai.embed_content(
                model=self.embedding_model,
                content=text,
                task_type="retrieval_document"  # ç”¨æ–¼æ–‡æª”å„²å­˜
            )
            return result['embedding']
        except Exception as e:
            self.logger.error(f"Embedding generation error: {e}", exc_info=True)
            raise EmbeddingError(f"Failed to generate embedding: {e}")
    
    def generate_query_embedding(self, query: str) -> List[float]:
        """
        ç‚ºæœå°‹æŸ¥è©¢ç”Ÿæˆ embedding å‘é‡
        
        Args:
            query: æœå°‹æŸ¥è©¢æ–‡æœ¬
            
        Returns:
            è¡¨ç¤º embedding å‘é‡çš„æµ®é»æ•¸åˆ—è¡¨ï¼ˆ768 ç¶­ï¼‰
            
        Raises:
            EmbeddingError: å¦‚æœ embedding ç”Ÿæˆå¤±æ•—
        """
        try:
            result = genai.embed_content(
                model=self.embedding_model,
                content=query,
                task_type="retrieval_query"  # ç”¨æ–¼æŸ¥è©¢æª¢ç´¢
            )
            return result['embedding']
        except Exception as e:
            self.logger.error(f"Query embedding generation error: {e}", exc_info=True)
            raise EmbeddingError(f"Failed to generate query embedding: {e}")
    
    def batch_generate_embeddings(self, texts: List[str]) -> List[Optional[List[float]]]:
        """
        ç‚ºå¤šå€‹æ–‡æœ¬ç”Ÿæˆ embeddings
        
        Args:
            texts: è¦åµŒå…¥çš„æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            embedding å‘é‡åˆ—è¡¨
        """
        embeddings = []
        for text in texts:
            embedding = self.generate_embedding(text)
            embeddings.append(embedding)
        return embeddings
    
    @staticmethod
    def cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
        """
        è¨ˆç®—å…©å€‹å‘é‡ä¹‹é–“çš„é¤˜å¼¦ç›¸ä¼¼åº¦
        
        Args:
            vec1: ç¬¬ä¸€å€‹å‘é‡
            vec2: ç¬¬äºŒå€‹å‘é‡
            
        Returns:
            é¤˜å¼¦ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        """
        v1 = np.array(vec1)
        v2 = np.array(vec2)
        
        dot_product = np.dot(v1, v2)
        norm1 = np.linalg.norm(v1)
        norm2 = np.linalg.norm(v2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return float(dot_product / (norm1 * norm2))
```

**é‡é»èªªæ˜ï¼š**
1. **ä¸åŒ Task Types**ï¼šæ–‡æª”ä½¿ç”¨ `retrieval_document`ï¼ŒæŸ¥è©¢ä½¿ç”¨ `retrieval_query`
2. **éŒ¯èª¤è™•ç†**ï¼šä½¿ç”¨è‡ªè¨‚ `EmbeddingError` ç•°å¸¸
3. **æ‰¹æ¬¡è™•ç†**ï¼šæ”¯æ´æ‰¹æ¬¡ç”Ÿæˆ embeddings
4. **å‘é‡è¨ˆç®—**ï¼šæä¾›é¤˜å¼¦ç›¸ä¼¼åº¦éœæ…‹æ–¹æ³•

---

## 6. é€²éšæ‡‰ç”¨

### 6.1 å‘é‡ç›¸ä¼¼åº¦æœå°‹ï¼ˆRAG æ ¸å¿ƒï¼‰

æœ¬å°ˆæ¡ˆä½¿ç”¨ PostgreSQL + pgvector æ“´å……å¯¦ç¾å‘é‡æœå°‹ã€‚

**æª”æ¡ˆï¼š** `backend/services/document_service.py` (ç¬¬ 149-194 è¡Œ)

```python
def search_similar_documents(
    self,
    query: str,
    top_k: int = 5,
    similarity_threshold: float = 0.7
) -> List[Tuple[Document, float]]:
    """
    ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦æœå°‹èˆ‡æŸ¥è©¢ç›¸ä¼¼çš„æ–‡æª”
    
    Args:
        query: æœå°‹æŸ¥è©¢æ–‡æœ¬
        top_k: è¦è¿”å›çš„é ‚éƒ¨çµæœæ•¸é‡
        similarity_threshold: æœ€å°ç›¸ä¼¼åº¦åˆ†æ•¸ (0-1)
        
    Returns:
        (document, similarity_score) å…ƒçµ„åˆ—è¡¨
        
    Raises:
        EmbeddingError: å¦‚æœæŸ¥è©¢ embedding ç”Ÿæˆå¤±æ•—
        DatabaseError: å¦‚æœæœå°‹å¤±æ•—
    """
    try:
        # ç”ŸæˆæŸ¥è©¢ embeddingï¼ˆæœƒåœ¨å¤±æ•—æ™‚æ‹‹å‡º EmbeddingErrorï¼‰
        query_embedding = self.embedding_service.generate_query_embedding(query)
        
        # ä½¿ç”¨ pgvector åŸ·è¡Œå‘é‡ç›¸ä¼¼åº¦æœå°‹
        # ä½¿ç”¨é¤˜å¼¦è·é›¢ (1 - cosine similarity)
        results = self.db.query(
            Document,
            (1 - Document.embedding.cosine_distance(query_embedding)).label('similarity')
        ).filter(
            Document.embedding.isnot(None)
        ).order_by(
            text('similarity DESC')
        ).limit(top_k).all()
        
        # ä¾ç›¸ä¼¼åº¦é–¾å€¼éæ¿¾
        filtered_results = [
            (doc, float(sim)) for doc, sim in results 
            if float(sim) >= similarity_threshold
        ]
        
        self.logger.info(f"Search found {len(filtered_results)} results for query")
        return filtered_results
    
    except EmbeddingError:
        raise
    except Exception as e:
        self.logger.error(f"Error searching documents: {e}", exc_info=True)
        raise DatabaseError(f"Failed to search documents: {e}")
```

**è³‡æ–™åº« Schemaï¼š** `backend/database/models.py` (ç¬¬ 7-30 è¡Œ)

```python
from sqlalchemy import Column, Integer, String, Text, TIMESTAMP, func
from sqlalchemy.ext.declarative import declarative_base
from pgvector.sqlalchemy import Vector

Base = declarative_base()

class Document(Base):
    """å…·æœ‰å‘é‡ embeddings çš„æ–‡æª”æ¨¡å‹"""
    __tablename__ = 'documents'

    id = Column(Integer, primary_key=True, index=True)
    gemini_file_name = Column(String(255), unique=True, nullable=False, index=True)
    display_name = Column(String(255), nullable=False)
    content = Column(Text, nullable=False)
    embedding = Column(Vector(768))  # Gemini text-embedding-004 (768 ç¶­)
    file_size = Column(Integer)
    created_at = Column(TIMESTAMP, server_default=func.now())
    updated_at = Column(TIMESTAMP, server_default=func.now(), onupdate=func.now())
```

**è³‡æ–™åº«ç´¢å¼•ï¼š** `backend/database/init.sql` (ç¬¬ 17 è¡Œ)

```sql
-- ç‚ºå‘é‡æœå°‹å»ºç«‹ IVFFlat ç´¢å¼•
CREATE INDEX IF NOT EXISTS documents_embedding_idx 
ON documents USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);
```

**é‡é»èªªæ˜ï¼š**
1. **pgvector æ“´å……**ï¼šPostgreSQL çš„å‘é‡æœå°‹æ“´å……
2. **IVFFlat ç´¢å¼•**ï¼šåŠ é€Ÿå‘é‡æœå°‹ï¼ˆé©åˆä¸­ç­‰è¦æ¨¡è³‡æ–™é›†ï¼‰
3. **Cosine Distance**ï¼š`1 - cosine_similarity` è½‰æ›ç‚ºè·é›¢åº¦é‡
4. **é–¾å€¼éæ¿¾**ï¼šåªè¿”å›ç›¸ä¼¼åº¦ â‰¥ threshold çš„çµæœ

### 6.2 å®Œæ•´ RAG æµç¨‹

```
ä½¿ç”¨è€…æŸ¥è©¢ "èª°æœ‰ CISSP è­‰ç…§ï¼Ÿ"
        â†“
1. ç”ŸæˆæŸ¥è©¢ embedding
   [EmbeddingService.generate_query_embedding()]
        â†“
2. å‘é‡ç›¸ä¼¼åº¦æœå°‹
   [DocumentService.search_similar_documents()]
   â†’ æ‰¾åˆ° Top 5 æœ€ç›¸é—œæ–‡æª”
        â†“
3. çµ„åˆæç¤ºè©
   [RAGService.query()]
   â†’ æ–‡ä»¶å…§å®¹ + ç”¨æˆ¶æŸ¥è©¢
        â†“
4. å‘¼å« Gemini API
   [genai.GenerativeModel.generate_content()]
        â†“
5. è¿”å›ç­”æ¡ˆ
   "Alice Johnson å’Œ Grace Lee æœ‰ CISSP è­‰ç…§"
```

### 6.3 ç³»çµ±æç¤ºè©è¨­è¨ˆ

**é è¨­æç¤ºè©ï¼š** `backend/services/rag_service.py` (ç¬¬ 221-225 è¡Œ)

```python
default_system_prompt = """Based on the provided document content, please answer the following question:

{query}

If the documents don't contain relevant information, please state that clearly and provide a general answer."""
```

**è‡ªè¨‚æç¤ºè©ç¯„ä¾‹ï¼š**

```python
custom_prompt = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„äººåŠ›è³‡æºåˆ†æå¸«ã€‚
æ ¹æ“šæä¾›çš„å“¡å·¥è³‡æ–™ï¼Œå›ç­”ä»¥ä¸‹å•é¡Œï¼š

{query}

è«‹ä»¥æ¢åˆ—å¼å‘ˆç¾çµæœï¼Œä¸¦åŒ…å«ï¼š
1. ç¬¦åˆæ¢ä»¶çš„äººå“¡å§“å
2. ç›¸é—œè­‰ç…§èˆ‡å¹´è³‡
3. å»ºè­°çš„è·ä½æˆ–å°ˆæ¡ˆé…ç½®

å¦‚æœè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"""

response = rag_service.query(
    query="èª°é©åˆæ“”ä»»è³‡å®‰ä¸»ç®¡ï¼Ÿ",
    system_prompt=custom_prompt
)
```

---

## 7. æœ€ä½³å¯¦è¸

### 7.1 æˆæœ¬å„ªåŒ–

#### âŒ ä¸è‰¯åšæ³•ï¼ˆé«˜æˆæœ¬ï¼‰
```python
# æ¯æ¬¡éƒ½å‚³é€æ‰€æœ‰æ–‡ä»¶
all_files = rag_service.list_files()
all_file_names = [f['name'] for f in all_files]

response = rag_service.query(
    query="èª°æœ‰ CISSPï¼Ÿ",
    selected_file_names=all_file_names  # å‚³é€ 20 å€‹æ–‡ä»¶ï¼
)
# Token æ¶ˆè€—: ~52,000
# æˆæœ¬: ~$0.039/æ¬¡
```

#### âœ… è‰¯å¥½åšæ³•ï¼ˆä½æˆæœ¬ï¼‰
```python
# ä½¿ç”¨å‘é‡æœå°‹åªå–ç›¸é—œæ–‡ä»¶
similar_docs = doc_service.search_similar_documents(
    query="èª°æœ‰ CISSPï¼Ÿ",
    top_k=5,
    similarity_threshold=0.6
)

selected_files = [doc.gemini_file_name for doc, score in similar_docs]

response = rag_service.query(
    query="èª°æœ‰ CISSPï¼Ÿ",
    selected_file_names=selected_files  # åªå‚³é€ 5 å€‹ç›¸é—œæ–‡ä»¶
)
# Token æ¶ˆè€—: ~8,000-13,000
# æˆæœ¬: ~$0.006-0.01/æ¬¡
# ç¯€çœ: 75%
```

### 7.2 éŒ¯èª¤è™•ç†

```python
from backend.exceptions import ModelValidationError, EmbeddingError, DatabaseError

try:
    # å˜—è©¦ç”Ÿæˆå›æ‡‰
    response = rag_service.query(
        query=user_query,
        model_name=selected_model
    )
    
    if response['success']:
        print(response['response'])
    else:
        print(f"æŸ¥è©¢å¤±æ•—: {response.get('error')}")
        
except ModelValidationError as e:
    print(f"æ¨¡å‹ä¸æ”¯æ´: {e}")
    
except EmbeddingError as e:
    print(f"Embedding ç”Ÿæˆå¤±æ•—: {e}")
    
except DatabaseError as e:
    print(f"è³‡æ–™åº«éŒ¯èª¤: {e}")
    
except Exception as e:
    print(f"æœªé æœŸçš„éŒ¯èª¤: {e}")
```

### 7.3 æ—¥èªŒè¨˜éŒ„

å°ˆæ¡ˆä½¿ç”¨çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„æ‰€æœ‰é‡è¦æ“ä½œã€‚

**æª”æ¡ˆï¼š** `backend/utils/logger.py`

```python
import logging
import sys

def get_logger(name: str) -> logging.Logger:
    """å–å¾—é…ç½®å¥½çš„ logger"""
    logger = logging.getLogger(name)
    
    if not logger.handlers:
        logger.setLevel(logging.INFO)
        
        # Console handler
        handler = logging.StreamHandler(sys.stdout)
        handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        
        logger.addHandler(handler)
    
    return logger
```

**ä½¿ç”¨ç¯„ä¾‹ï¼š**

```python
from backend.utils.logger import get_logger

logger = get_logger(__name__)

logger.info("Starting document upload...")
logger.warning("Similarity threshold too low, adjusting...")
logger.error("API call failed", exc_info=True)  # åŒ…å« stack trace
```

### 7.4 æŸ¥è©¢æ—¥èªŒèˆ‡çµ±è¨ˆ

å°ˆæ¡ˆè¨˜éŒ„æ¯æ¬¡æŸ¥è©¢çš„è©³ç´°è³‡è¨Šç”¨æ–¼åˆ†æã€‚

**æª”æ¡ˆï¼š** `backend/services/document_service.py` (ç¬¬ 196-230 è¡Œ)

```python
def log_query(
    self,
    query: str,
    model_used: str,
    files_used: int = 0,
    selected_files: Optional[List[str]] = None,
    system_prompt_used: Optional[str] = None,
    response_length: Optional[int] = None,
    prompt_tokens: Optional[int] = None,
    completion_tokens: Optional[int] = None,
    total_tokens: Optional[int] = None,
    success: bool = True,
    error_message: Optional[str] = None
) -> QueryLog:
    """
    è¨˜éŒ„æŸ¥è©¢ç”¨æ–¼ä½¿ç”¨çµ±è¨ˆ
    
    Raises:
        DatabaseError: å¦‚æœè¨˜éŒ„å¤±æ•—
    """
    try:
        log = QueryLog(
            query=query,
            model_used=model_used,
            files_used=files_used,
            selected_files=selected_files or [],
            system_prompt_used=system_prompt_used,
            response_length=response_length,
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens,
            success=success,
            error_message=error_message
        )
        
        self.db.add(log)
        self.db.commit()
        self.db.refresh(log)
        
        return log
    
    except Exception as e:
        self.db.rollback()
        self.logger.error(f"Error logging query: {e}", exc_info=True)
        raise DatabaseError(f"Failed to log query: {e}")
```

**çµ±è¨ˆåˆ†æï¼š** å¯æŸ¥è©¢å¹³å‡ token ä½¿ç”¨é‡ã€æˆåŠŸç‡ã€ç†±é–€æ¨¡å‹ç­‰ã€‚

---

## 8. å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•é¸æ“‡åˆé©çš„æ¨¡å‹ï¼Ÿ

**A:** æ ¹æ“šéœ€æ±‚é¸æ“‡ï¼š

| éœ€æ±‚ | æ¨è–¦æ¨¡å‹ | ç†ç”± |
|------|---------|------|
| å¿«é€Ÿå›æ‡‰ + ä½æˆæœ¬ | `gemini-1.5-flash` | é€Ÿåº¦å¿«ï¼Œæˆæœ¬ä½ |
| è¤‡é›œæ¨ç† | `gemini-1.5-pro` | æ¨ç†èƒ½åŠ›å¼· |
| æ¸¬è©¦æ–°åŠŸèƒ½ | `gemini-2.0-flash-exp` | æœ€æ–°å¯¦é©—åŠŸèƒ½ |
| ç”Ÿç”¢ç’°å¢ƒ | `gemini-1.5-flash` | ç©©å®šå¯é  |

### Q2: Embedding æ¨¡å‹å¯ä»¥æ›´æ›å—ï¼Ÿ

**A:** å¯ä»¥ï¼Œä½†éœ€æ³¨æ„ï¼š

1. **ç¶­åº¦å¿…é ˆä¸€è‡´**ï¼šè³‡æ–™åº« schema å®šç¾©ç‚º 768 ç¶­
2. **éœ€è¦é‡æ–°ç”Ÿæˆ**ï¼šæ‰€æœ‰æ–‡æª”çš„ embedding éƒ½è¦é‡æ–°è¨ˆç®—
3. **æ•ˆèƒ½å½±éŸ¿**ï¼šä¸åŒæ¨¡å‹çš„èªç¾©ç†è§£èƒ½åŠ›ä¸åŒ

```python
# å¦‚è¦æ›´æ›æ¨¡å‹
EMBEDDING_MODEL = "models/text-embedding-005"  # å‡è¨­æœªä¾†æ¨å‡º

# éœ€è¦æ›´æ–°è³‡æ–™åº« schema
# ALTER TABLE documents ALTER COLUMN embedding TYPE vector(NEW_DIM);
```

### Q3: ç›¸ä¼¼åº¦é–¾å€¼å¦‚ä½•è¨­å®šï¼Ÿ

**A:** æ ¹æ“šæ‡‰ç”¨å ´æ™¯èª¿æ•´ï¼š

| é–¾å€¼ | é©ç”¨å ´æ™¯ | èªªæ˜ |
|------|---------|------|
| 0.9-1.0 | ç²¾ç¢ºåŒ¹é… | åªè¿”å›éå¸¸ç›¸é—œçš„æ–‡æª” |
| 0.7-0.9 | ä¸€èˆ¬æª¢ç´¢ï¼ˆæ¨è–¦ï¼‰ | å¹³è¡¡ç²¾ç¢ºåº¦èˆ‡å¬å›ç‡ |
| 0.5-0.7 | å¯¬é¬†æª¢ç´¢ | å¯èƒ½åŒ…å«å¼±ç›¸é—œæ–‡æª” |
| < 0.5 | æ¢ç´¢æ€§æœå°‹ | å¤§é‡çµæœï¼Œéœ€äººå·¥ç¯©é¸ |

**å°ˆæ¡ˆé è¨­ï¼š** `0.7`ï¼ˆåœ¨ `backend/config.py` æˆ–å‰ç«¯è¨­å®šï¼‰

### Q4: æ–‡ä»¶ä¸Šå‚³å¾Œå¤šä¹…å¯ä»¥æœå°‹ï¼Ÿ

**A:** å³æ™‚å¯æœå°‹ã€‚æµç¨‹ï¼š

```
ä¸Šå‚³æ–‡ä»¶ â†’ æå–å…§å®¹ â†’ ç”Ÿæˆ Embedding â†’ å­˜å…¥è³‡æ–™åº«ï¼ˆwith å‘é‡ï¼‰
                                                â†“
                                            ç«‹å³å¯æœå°‹
```

æ™‚é–“ï¼šç´„ 1-3 ç§’/æ–‡ä»¶ï¼ˆå–æ±ºæ–¼æ–‡ä»¶å¤§å°ï¼‰

### Q5: å¦‚ä½•è™•ç†å¤§å‹æ–‡ä»¶ï¼Ÿ

**A:** ç›®å‰å°ˆæ¡ˆå°‡æ•´å€‹æ–‡ä»¶ä½œç‚ºä¸€å€‹æ–‡æª”ï¼Œå»ºè­°å„ªåŒ–ï¼š

#### æ–¹æ¡ˆ Aï¼šæ–‡ä»¶åˆ†å¡Šï¼ˆChunkingï¼‰

```python
def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:
    """å°‡æ–‡æœ¬åˆ‡å‰²æˆé‡ç–Šçš„å¡Š"""
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        chunks.append(chunk)
        start = end - overlap
    
    return chunks

# ä½¿ç”¨
content = read_file('large_document.txt')
chunks = chunk_text(content, chunk_size=500)

for i, chunk in enumerate(chunks):
    doc_service.create_document(
        gemini_file_name=f'doc_chunk_{i}',
        display_name=f'Document Part {i+1}',
        content=chunk
    )
```

#### æ–¹æ¡ˆ Bï¼šæ®µè½ç´šæª¢ç´¢

```python
# ä¾æ®µè½åˆ†å‰²
paragraphs = content.split('\n\n')

for i, para in enumerate(paragraphs):
    if para.strip():
        doc_service.create_document(
            gemini_file_name=f'doc_para_{i}',
            display_name=f'Paragraph {i+1}',
            content=para
        )
```

### Q6: API é…é¡è¶…é™æ€éº¼è¾¦ï¼Ÿ

**éŒ¯èª¤è¨Šæ¯ï¼š**
```
google.api_core.exceptions.ResourceExhausted: 429 Resource has been exhausted
```

**è§£æ±ºæ–¹æ¡ˆï¼š**

1. **é™ä½è«‹æ±‚é »ç‡**ï¼šåŠ å…¥ rate limiting
2. **ä½¿ç”¨å…¶ä»–æ¨¡å‹**ï¼šæŸäº›æ¨¡å‹æœ‰æ›´é«˜é…é¡
3. **å‡ç´š API æ–¹æ¡ˆ**ï¼šè¯çµ¡ Google å¢åŠ é…é¡
4. **å¯¦ä½œå¿«å–æ©Ÿåˆ¶**ï¼šç›¸åŒæŸ¥è©¢è¿”å›å¿«å–çµæœ

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=100)
def cached_query(query_hash: str) -> str:
    """å¿«å–æŸ¥è©¢çµæœ"""
    # å¯¦éš›æŸ¥è©¢é‚è¼¯
    pass

# ä½¿ç”¨
query_hash = hashlib.md5(user_query.encode()).hexdigest()
result = cached_query(query_hash)
```

### Q7: å¦‚ä½•ç›£æ§ token ä½¿ç”¨é‡ï¼Ÿ

**A:** å°ˆæ¡ˆå·²å¯¦ä½œå®Œæ•´çš„ token è¿½è¹¤ã€‚

```python
# æŸ¥è©¢æ™‚è‡ªå‹•è¨˜éŒ„
response = rag_service.query(query="...")

print(f"Prompt Tokens: {response['prompt_tokens']}")
print(f"Completion Tokens: {response['completion_tokens']}")
print(f"Total Tokens: {response['total_tokens']}")

# æŸ¥çœ‹çµ±è¨ˆ
stats = doc_service.get_query_stats()
print(f"Total Tokens Used: {stats.total_tokens_used}")
print(f"Average per Query: {stats.avg_tokens_per_query}")
```

### Q8: å‘é‡æœå°‹æ•ˆèƒ½å¦‚ä½•å„ªåŒ–ï¼Ÿ

**A:** å¤šç¨®å„ªåŒ–ç­–ç•¥ï¼š

#### 1. è³‡æ–™åº«ç´¢å¼•å„ªåŒ–

```sql
-- ç•¶å‰ä½¿ç”¨ IVFFlatï¼ˆé©åˆ < 100 è¬ç­†ï¼‰
CREATE INDEX documents_embedding_idx 
ON documents USING ivfflat (embedding vector_cosine_ops) 
WITH (lists = 100);

-- å¤§è¦æ¨¡è³‡æ–™ï¼ˆ> 100 è¬ç­†ï¼‰æ”¹ç”¨ HNSW
CREATE INDEX documents_embedding_idx 
ON documents USING hnsw (embedding vector_cosine_ops);
```

#### 2. èª¿æ•´ lists åƒæ•¸

```sql
-- lists = sqrt(ç¸½æ–‡æª”æ•¸) é€šå¸¸æ˜¯æœ€ä½³å€¼
-- 1000 ç­†æ–‡æª” â†’ lists = 32
-- 10000 ç­†æ–‡æª” â†’ lists = 100
-- 100000 ç­†æ–‡æª” â†’ lists = 316
```

#### 3. å®šæœŸ VACUUM

```sql
-- å®šæœŸæ¸…ç†èˆ‡é‡å»ºç´¢å¼•
VACUUM ANALYZE documents;
REINDEX INDEX documents_embedding_idx;
```

---

## ğŸ“– å»¶ä¼¸é–±è®€

### å®˜æ–¹æ–‡ä»¶
- [Google AI Python SDK](https://ai.google.dev/tutorials/python_quickstart)
- [Gemini API Documentation](https://ai.google.dev/gemini-api/docs)
- [Text Embeddings Guide](https://ai.google.dev/gemini-api/docs/embeddings)

### ç›¸é—œæŠ€è¡“
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [RAG (Retrieval-Augmented Generation)](https://arxiv.org/abs/2005.11401)
- [Vector Search Best Practices](https://www.pinecone.io/learn/vector-search/)

### å°ˆæ¡ˆç›¸é—œæª”æ¡ˆ
```
backend/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ rag_service.py          # Gemini API æ•´åˆ
â”‚   â”œâ”€â”€ embedding_service.py    # Embedding ç”Ÿæˆ
â”‚   â””â”€â”€ document_service.py     # å‘é‡æœå°‹
â”œâ”€â”€ routers/
â”‚   â”œâ”€â”€ chat.py                 # èŠå¤© API ç«¯é»
â”‚   â””â”€â”€ search.py               # æœå°‹ API ç«¯é»
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ models.py               # è³‡æ–™åº« schema
â”‚   â””â”€â”€ init.sql                # åˆå§‹åŒ– SQL
â””â”€â”€ config.py                   # é…ç½®å¸¸æ•¸
```

---

## ğŸ¯ ç¸½çµ

æœ¬æ–‡ä»¶æ¶µè“‹äº† Google Generative AI SDK çš„æ ¸å¿ƒåŠŸèƒ½ï¼Œä¸¦çµåˆå¯¦éš›å°ˆæ¡ˆå±•ç¤ºï¼š

âœ… **åŸºç¤è¨­ç½®**ï¼šAPI key é…ç½®èˆ‡åˆå§‹åŒ–  
âœ… **æ¨¡å‹ç®¡ç†**ï¼šå‹•æ…‹è¼‰å…¥èˆ‡é¸æ“‡æ¨¡å‹  
âœ… **æ–‡ä»¶ç®¡ç†**ï¼šä¸Šå‚³ã€åˆ—è¡¨ã€åˆªé™¤  
âœ… **å…§å®¹ç”Ÿæˆ**ï¼šæ™®é€šèˆ‡ä¸²æµæŸ¥è©¢  
âœ… **Embedding**ï¼šæ–‡æª”èˆ‡æŸ¥è©¢å‘é‡åŒ–  
âœ… **å‘é‡æœå°‹**ï¼šPostgreSQL + pgvector  
âœ… **RAG æ‡‰ç”¨**ï¼šå®Œæ•´çš„æª¢ç´¢å¢å¼·ç”Ÿæˆæµç¨‹  
âœ… **æœ€ä½³å¯¦è¸**ï¼šæˆæœ¬å„ªåŒ–ã€éŒ¯èª¤è™•ç†ã€æ—¥èªŒè¨˜éŒ„  

å¸Œæœ›é€™ä»½æ•™å­¸èƒ½å¹«åŠ©æ‚¨å¿«é€Ÿä¸Šæ‰‹ Gemini API ä¸¦æ‡‰ç”¨åˆ°å¯¦éš›å°ˆæ¡ˆä¸­ï¼
